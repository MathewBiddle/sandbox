{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests and set the OBIS API base URL. \n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import urllib\n",
    "\n",
    "# Convenience function to pretty print JSON objects\n",
    "def print_json(myjson):\n",
    "    print(json.dumps(\n",
    "        myjson,\n",
    "        sort_keys=True,\n",
    "        indent=4,\n",
    "        separators=(',', ': ')\n",
    "    ))\n",
    "    \n",
    "\n",
    "# Initialize the base URL for OBIS. This variable will be used for every API call\n",
    "OBIS_URL = \"https://api.obis.org/v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not sure which node ID to query so lets get all of the OTN nodes.\n",
    "\n",
    "# node\n",
    "req = requests.get(f'{OBIS_URL}/node')\n",
    "nodes_json = req.json()\n",
    "\n",
    "# count the amount of OBIS nodes\n",
    "f\"Total Nodes: {nodes_json['total']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints all the names and IDs for each node\n",
    "for node in nodes_json['results']:\n",
    "    print(f'Name: {node[\"name\"]} - ID: {node[\"id\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh look, the OBIS USA is a OBIS node, lets just return our record using the id value:\n",
    "nodeID = 'b7c47783-a020-4173-b390-7b57c4fa1426'\n",
    "# node/{nodeID}\n",
    "req = requests.get(f'{OBIS_URL}/node/{nodeID}')\n",
    "obis_usa_json = req.json()\n",
    "\n",
    "# Show OBIS-USA node record\n",
    "print_json(obis_usa_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(f'{OBIS_URL}/dataset?nodeid={nodeID}')\n",
    "datasets = req.json()\n",
    "print('Number of datasets in OBIS-USA:', datasets['total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-princeton",
   "metadata": {},
   "source": [
    "Lets print out the metadata from one of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_json(datasets['results'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-announcement",
   "metadata": {},
   "source": [
    "Now, lets iterate through all the datasets and collect metadata into a Pandas DataFrame. We're skipping over the ipt from ipt.geome-db because the website doesn't load: https://ipt.geome-db.org/resource?r=dipnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets grab out some metadata about each dataset\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "columns = ['title','url','size_raw','size_MB']\n",
    "\n",
    "df = pd.DataFrame(\n",
    "        columns=columns\n",
    "    )\n",
    "\n",
    "for dataset in datasets['results']:\n",
    "    if 'ipt.geome-db.org' not in dataset['url']:\n",
    "        print(dataset['title'])\n",
    "        print(dataset['url'])\n",
    "        html_text = requests.get(dataset['url']).text\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        \n",
    "        size_raw = soup.find('td').text.split('(')[1].split(')')[0]\n",
    "        size = float(size_raw.split(\" \")[0].replace(\",\",\"\"))\n",
    "        size_unit = size_raw.split(\" \")[1]\n",
    "        \n",
    "        #convert sizes to MB\n",
    "        if size_unit == 'KB':\n",
    "            size = size*0.001\n",
    "        elif size_unit == 'MB':\n",
    "            size = size\n",
    "        \n",
    "        df_init = pd.DataFrame(\n",
    "                    {\"title\": dataset['title'],\n",
    "                     \"url\": dataset['url'],\n",
    "                     \"size_raw\": size_raw,\n",
    "                     \"size_MB\": size,\n",
    "                     },\n",
    "                  index=[1])\n",
    "\n",
    "        df = pd.concat([df, df_init], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-photography",
   "metadata": {},
   "source": [
    "Print out statistics about the package sizes (in MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sum\\t',df['size_MB'].sum())\n",
    "print(df['size_MB'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-cover",
   "metadata": {},
   "source": [
    "## Download each Darwin Core Archive package\n",
    "Don't download if it's already on local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for url in df['url']:\n",
    "    print(url)\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    size_raw = soup.find('td')\n",
    "    \n",
    "    url_download = size_raw.find('a').get('href')\n",
    "    fname = 'OBIS_data/'+url.split('=')[-1]+'.zip'\n",
    "    \n",
    "    if not os.path.exists(fname):\n",
    "        print('Downloading '+url)\n",
    "        urllib.request.urlretrieve(url_download, fname)\n",
    "        print('Downloaded to '+fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-pakistan",
   "metadata": {},
   "source": [
    "Read the occurrence data from all the packages you just downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# occurrence1 = pd.DataFrame(\n",
    "#         columns = ['id', 'type', 'basisOfRecord', 'occurrenceID', 'occurrenceStatus',\n",
    "#        'eventID', 'eventDate', 'decimalLatitude', 'decimalLongitude',\n",
    "#        'scientificNameID', 'scientificName', 'kingdom', 'phylum', 'class',\n",
    "#        'order', 'family', 'genus', 'taxonRank', 'scientificNameAuthorship'])\n",
    "\n",
    "# from zipfile import ZipFile\n",
    "# for obis_zip in os.listdir('OBIS_data/'):\n",
    "#     if not obis_zip == 'unzipped':\n",
    "#         with ZipFile('OBIS_data/'+obis_zip,'r') as zip:\n",
    "#             df_init = pd.read_csv(obis_zip.open('occurrence.txt'), sep='\\t') # not every occurrence file has eventDate\n",
    "#             # extract all zip packages\n",
    "#             # zip.extract_all(path='OBIS_data/unzipped/'+obis_zip.replace('.zip','/'))\n",
    "#             # zip.ZIP_STORED\n",
    "#             # might be able to read into pandas\n",
    "#             occurrence1 = pd.concat([occurrence1, df_init], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-whale",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "systematic-catalog",
   "metadata": {},
   "source": [
    "Try to use the darwin core python reader package from https://python-dwca-reader.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DwCAReader('OBIS_data/ambon_cetaceans_2015.zip') as dwca:\n",
    "    print(dwca.archive_path)\n",
    "    root = dwca.metadata\n",
    "    node = root.find('.//westBoundingCoordinate')\n",
    "    print('%s: %s' % (node.tag, node.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-vessel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "finnish-strain",
   "metadata": {},
   "source": [
    "Now lets do some automated ingest of all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dwca.read import DwCAReader\n",
    "from dwca.darwincore.utils import qualname as qn\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "core_df = pd.DataFrame()\n",
    "# occurrence only = OBIS_data/wod_2009.zip\n",
    "# event = OBIS_data/ambon_cetaceans_2015.zip\n",
    "for obis_zip in os.listdir('OBIS_data/'):\n",
    "    if not obis_zip == 'unzipped':\n",
    "        with DwCAReader('OBIS_data/'+obis_zip) as dwca:\n",
    "            #eml = dwca.metadata\n",
    "            print(\"\\nReading: %s\" % dwca.archive_path)\n",
    "            print(\"Core type is: %s\" % dwca.descriptor.core.type)\n",
    "            print(\"Core data file is: %s\" % dwca.descriptor.core.file_location)\n",
    "            for ex in dwca.descriptor.extensions:\n",
    "                print('Extensions: ',ex.type)\n",
    "\n",
    "            core_df = pd.concat(\n",
    "                [core_df, dwca.pd_read(dwca.core_file_location, parse_dates = True)], \n",
    "                axis = 0, \n",
    "                ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xml.etree.ElementTree as ET\n",
    "print(dwca.archive_path)\n",
    "root = dwca.metadata\n",
    "for child in root.findall('.//role'):\n",
    "    print(child.tag, child.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-craft",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#core_df['eventDate'].filter(like='0001', axis=0)\n",
    "import numpy as np\n",
    "\n",
    "## non-convertable dates:\n",
    "bad_dates = [\"0001-05-17\",\"0001-04-11\",\"0001-05-17\",\"0001-04-11\",\"0001-04-11\",\"0001-04-11\",\"0193-09-10\",\"0193-09-10\",\"0001-05-18\",\"1027-10-24\",\"0001-04-11\",\"0001-04-11\",\"0001-05-18\",\"0001-04-11\",\"0001-05-18\",\"0001-04-11\",\"0001-05-17\",\"0001-04-11\",\"0001-04-11\",\"0001-04-11\",\"0001-07-30\",\"0001-07-30\",\"0001-04-11\",\"0001-05-17\",\"0001-04-11\",\"0001-05-17\",\"2000-07-18\",\"2006-10-05T11:55\",\"3291-01-27\",\"1975-10-05T20:15:00Z\",\"1981-05-10T23:10:12Z\",\"1985-07-09T12:00:00Z\",\"1977-11-03T12:00:00Z\",\"1988-10-09T02:15:00Z\",\"1989-09-24T08:58:12Z\",\"1995-01-27T03:01:48Z\",\"1967-02-07T07:10:12Z\"]\n",
    "# replace those w/ nan\n",
    "core_df['eventDate'].replace(bad_dates,np.nan, inplace = True)\n",
    "# 164,341 observations have null dates\n",
    "pd.to_datetime(core_df['eventDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of bad dates\n",
    "core_df.loc[core_df['eventDate'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(core_df['eventDate'],infer_datetime_format=True).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in core_df['eventDate'].values:\n",
    "    try:\n",
    "        pd.to_datetime(value)\n",
    "    except:\n",
    "        print(value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sizes = pd.DataFrame({\n",
    "    'size':[0.015835,\n",
    "0.018933,\n",
    "0.049817,\n",
    "0.087867,\n",
    "0.110903,\n",
    "0.159178,\n",
    "0.197299,\n",
    "0.241113,\n",
    "0.297959,\n",
    "0.415823,\n",
    "0.457184,\n",
    "0.510765,\n",
    "0.590182,\n",
    "0.601713,\n",
    "0.703488,\n",
    "0.703526,\n",
    "0.703676,\n",
    "0.703744,\n",
    "0.703871,\n",
    "0.703871,\n",
    "0.777377,\n",
    "0.924155,\n",
    "1.10372,\n",
    "1.44756,\n",
    "1.56119,\n",
    "1.56151,\n",
    "1.5618,\n",
    "1.56186,\n",
    "1.90727,\n",
    "2.2512,\n",
    "2.25399,\n",
    "2.25418,\n",
    "2.25419,\n",
    "2.25465,\n",
    "2.25466,\n",
    "2.2547,\n",
    "2.25517,\n",
    "2.25533,\n",
    "2.25653,\n",
    "2.27472,\n",
    "2.38664,\n",
    "2.77899,\n",
    "3.26323,\n",
    "3.49174,\n",
    "3.73588,\n",
    "3.91314,\n",
    "4.10337,\n",
    "4.11706,\n",
    "4.37327,\n",
    "4.73277,\n",
    "5.53673,\n",
    "5.80743,\n",
    "6.01187,\n",
    "6.01363,\n",
    "6.01561,\n",
    "6.02871,\n",
    "6.03303,\n",
    "6.0342,\n",
    "6.21464,\n",
    "6.33658,\n",
    "6.50698,\n",
    "6.60439,\n",
    "6.67536,\n",
    "6.83223,\n",
    "7.04321,\n",
    "9.09705,\n",
    "9.58163,\n",
    "9.70774,\n",
    "9.84124,\n",
    "9.84241,\n",
    "10.5076,\n",
    "10.598,\n",
    "10.7722,\n",
    "11.7014,\n",
    "12.8499,\n",
    "12.8959,\n",
    "14.8624,\n",
    "16.0524,\n",
    "16.512,\n",
    "17.2536,\n",
    "18.5005,\n",
    "19.8645,\n",
    "23.5451,\n",
    "23.961,\n",
    "26.2298,\n",
    "26.3219,\n",
    "29.0074,\n",
    "29.0466,\n",
    "31.8277,\n",
    "34.5672,\n",
    "34.9802,\n",
    "36.9862,\n",
    "41.1389,\n",
    "46.6154,\n",
    "51.1379,\n",
    "54.0095,\n",
    "60.7156,\n",
    "61.032,\n",
    "78.5715,\n",
    "78.7834,\n",
    "82.6558,\n",
    "94.6736,\n",
    "96.1423,\n",
    "98.5113,\n",
    "116.51,\n",
    "121.919,\n",
    "127.061,\n",
    "161.56,\n",
    "167.974,\n",
    "173.018,\n",
    "173.473,\n",
    "182.459,\n",
    "190.971,\n",
    "198.847,\n",
    "203.816,\n",
    "206.923,\n",
    "212.72,\n",
    "221.919,\n",
    "230.564,\n",
    "234.129,\n",
    "239.094,\n",
    "245.25,\n",
    "245.448,\n",
    "251.73,\n",
    "254.732,\n",
    "257.656,\n",
    "266.012,\n",
    "272.378,\n",
    "286.562,\n",
    "287.254,\n",
    "296.639,\n",
    "303.077,\n",
    "306.597,\n",
    "324.479,\n",
    "325.387,\n",
    "337.982,\n",
    "354.244,\n",
    "373.063,\n",
    "416.506,\n",
    "477.162,\n",
    "612.461,\n",
    "804.789,\n",
    "844.7,\n",
    "1058.94,\n",
    "3681.96]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sizes.sum())\n",
    "print(df_sizes.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-directive",
   "metadata": {},
   "source": [
    "Read in the unpackaged occurrence data from `OBIS_data/unzipped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence = pd.DataFrame(\n",
    "        columns = ['id', 'type', 'basisOfRecord', 'occurrenceID', 'occurrenceStatus',\n",
    "       'eventID', 'eventDate', 'decimalLatitude', 'decimalLongitude',\n",
    "       'scientificNameID', 'scientificName', 'kingdom', 'phylum', 'class',\n",
    "       'order', 'family', 'genus', 'taxonRank', 'scientificNameAuthorship'])\n",
    "\n",
    "for package in os.listdir('OBIS_data/unzipped/'):\n",
    "    print('reading',package)\n",
    "    \n",
    "    df_init = pd.read_csv('OBIS_data/unzipped/'+package+'/occurrence.txt', sep = '\\t')\n",
    "    occurrence = pd.concat([occurrence, df_init], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(occurrence['eventDate']).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrence['eventDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(core_df[['decimalLatitude','decimalLongitude']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df[['decimalLatitude','decimalLongitude']].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df[['decimalLatitude','decimalLongitude']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-domestic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
